{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(),'dataset')\n",
    "classes = os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\ac\\2023_10_03_10_36_34.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\fan\\2023_10_03_10_31_11.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\fridge\\2023_10_03_10_58_26.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\light\\2023_10_03_10_27_36.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\microwave\\2023_10_03_10_57_19.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\tv\\2023_10_03_10_38_54.wav\n",
      "c:\\Users\\shami\\OneDrive\\Desktop\\v2\\dataset\\washing machine\\2023_10_03_10_56_38.wav\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for clas in classes:\n",
    "    audio_files = os.path.join(dataset_path,clas)\n",
    "    for file in os.listdir(audio_files):\n",
    "        feature = []\n",
    "        file_path = os.path.join(dataset_path,clas,file)\n",
    "        print(file_path)\n",
    "        audio_data,sr = librosa.load(file_path)\n",
    "\n",
    "        rms = librosa.feature.rms(y=audio_data)\n",
    "        rms_mean = np.mean(rms,axis=1)\n",
    "        rms_median = np.median(rms,axis=1)\n",
    "        feature.extend([rms_mean[0],rms_median[0]])\n",
    "\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr)\n",
    "        spec_bw_mean = np.mean(spec_bw,axis=1)\n",
    "        spec_bw_median = np.median(spec_bw,axis=1)\n",
    "        feature.extend([spec_bw_mean[0],spec_bw_median[0]])\n",
    "\n",
    "        S = np.abs(librosa.stft(y=audio_data))\n",
    "        contrast = librosa.feature.spectral_contrast(S=S, sr=sr)\n",
    "        contrast_row_mean = np.mean(np.mean(contrast,axis=1))\n",
    "        contrast_col_mean = np.mean(np.mean(contrast,axis=0))\n",
    "        contrast_row_median = np.median(np.median(contrast,axis=1))\n",
    "        contrast_col_median = np.median(np.median(contrast,axis=0))\n",
    "        feature.extend([contrast_row_mean, contrast_col_mean, contrast_row_median, contrast_col_median])\n",
    "\n",
    "        chroma_cens = librosa.feature.chroma_cens(y=audio_data, sr=sr)\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=audio_data, sr=sr)\n",
    "        chroma_cens_row_mean = np.mean(np.mean(chroma_cens,axis=1))\n",
    "        chroma_cens_col_mean = np.mean(np.mean(chroma_cens,axis=0))\n",
    "        chroma_cens_row_median = np.median(np.median(chroma_cq,axis=1))\n",
    "        chroma_cens_col_median = np.median(np.median(chroma_cq,axis=0))\n",
    "        chroma_cq_row_mean = np.mean(np.mean(chroma_cens,axis=1))\n",
    "        chroma_cq_col_mean = np.mean(np.mean(chroma_cens,axis=0))\n",
    "        chroma_cq_row_median = np.median(np.median(chroma_cq,axis=1))\n",
    "        chroma_cq_col_median = np.median(np.median(chroma_cq,axis=0))\n",
    "        feature.extend([chroma_cens_row_mean, chroma_cens_col_mean, chroma_cens_row_median, chroma_cens_col_median, chroma_cq_row_mean, chroma_cq_col_mean, chroma_cq_row_median, chroma_cq_col_median])\n",
    "\n",
    "        cent = librosa.feature.spectral_centroid(y=audio_data, sr=sr)\n",
    "        cent_mean = np.mean(cent,axis=1)\n",
    "        cent_median = np.median(cent,axis=1)\n",
    "        feature.extend([cent_mean[0],cent_median[0]])\n",
    "\n",
    "        flatness = librosa.feature.spectral_flatness(y=audio_data)\n",
    "        flatness_mean = np.mean(flatness,axis=1)\n",
    "        flatness_median = np.median(flatness,axis=1)\n",
    "        feature.extend([flatness_mean[0],flatness_median[0]])\n",
    "\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)\n",
    "        rolloff_mean = np.mean(rolloff,axis=1)\n",
    "        rolloff_median = np.median(rolloff,axis=1)\n",
    "        feature.extend([rolloff_mean[0],rolloff_median[0]])\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data,sr=sr)\n",
    "        row_mean = np.mean(np.mean(mfcc,axis=1))\n",
    "        col_mean = np.mean(np.mean(mfcc,axis=0))\n",
    "        row_median = np.median(np.median(mfcc,axis=1))\n",
    "        col_median = np.median(np.median(mfcc,axis=0))\n",
    "        feature.extend([row_mean, col_mean, row_median, col_median])\n",
    "\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio_data)\n",
    "        zcr_mean = np.mean(zcr,axis=1)\n",
    "        zcr_median = np.median(zcr,axis=1)\n",
    "        feature.extend([zcr_mean[0],zcr_median[0]])\n",
    "\n",
    "        magnitude, phase = librosa.magphase(librosa.stft(y=audio_data))\n",
    "        sro = librosa.feature.spectral_rolloff(S=magnitude,sr=sr)\n",
    "        sro_mean = np.mean(sro,axis=1)\n",
    "        sro_median = np.median(sro,axis=1)\n",
    "        feature.extend([sro_mean[0],sro_median[0]])\n",
    "\n",
    "        pitches,magnitudes = librosa.core.piptrack(y=audio_data,sr=sr)\n",
    "        pitches_row_mean = np.mean(np.mean(pitches,axis=1))\n",
    "        pitches_col_mean = np.mean(np.mean(pitches,axis=0))\n",
    "        pitches_row_median = np.median(np.median(pitches,axis=1))\n",
    "        pitches_col_median = np.median(np.median(pitches,axis=0))\n",
    "        magnitudes_row_mean = np.mean(np.mean(magnitudes,axis=1))\n",
    "        magnitudes_col_mean = np.mean(np.mean(magnitudes,axis=0))\n",
    "        magnitudes_row_median = np.median(np.median(magnitudes,axis=1))\n",
    "        magnitudes_col_median = np.median(np.median(magnitudes,axis=0))\n",
    "        feature.extend([pitches_row_mean, pitches_col_mean, pitches_row_median, pitches_col_median, magnitudes_row_mean, magnitudes_col_mean, magnitudes_row_median, magnitudes_col_median])\n",
    "\n",
    "        c_stft = librosa.feature.chroma_stft(y=audio_data,sr=sr)\n",
    "        c_stft_row_mean = np.mean(np.mean(c_stft,axis=1))\n",
    "        c_stft_col_mean = np.mean(np.mean(c_stft,axis=0))\n",
    "        c_stft_row_median = np.median(np.median(c_stft,axis=1))\n",
    "        c_stft_col_median = np.median(np.median(c_stft,axis=0))\n",
    "        feature.extend([c_stft_row_mean,c_stft_col_mean,c_stft_row_median,c_stft_col_median])\n",
    "\n",
    "        y = librosa.effects.harmonic(audio_data)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sr)\n",
    "        tonnetz_row_mean = np.mean(np.mean(tonnetz,axis=1))\n",
    "        tonnetz_col_mean = np.mean(np.mean(tonnetz,axis=0))\n",
    "        tonnetz_row_median = np.median(np.median(tonnetz,axis=1))\n",
    "        tonnetz_col_median = np.median(np.median(tonnetz,axis=0))\n",
    "        feature.extend([tonnetz_row_mean,tonnetz_col_mean,tonnetz_row_median,tonnetz_col_median])\n",
    "\n",
    "        feature.append(clas)\n",
    "        \n",
    "        with open('features.csv','a',newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(feature)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./features.csv',header=None)\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1401, 46)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"features.csv\",header=None)\n",
    "x = df.iloc[:,:-1].values\n",
    "x += 1e-8\n",
    "y = df.iloc[:,-1].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Training Loss: 83.6126, Training Accuracy: 0.1562 Validation Loss: 32.1546, Validation Accuracy: 0.1821\n",
      "Epoch 2/50 Training Loss: 23.5563, Training Accuracy: 0.1866 Validation Loss: 15.1231, Validation Accuracy: 0.2036\n",
      "Epoch 3/50 Training Loss: 12.9119, Training Accuracy: 0.2036 Validation Loss: 13.3556, Validation Accuracy: 0.1714\n",
      "Epoch 4/50 Training Loss: 8.7414, Training Accuracy: 0.1920 Validation Loss: 6.5310, Validation Accuracy: 0.1821\n",
      "Epoch 5/50 Training Loss: 7.0491, Training Accuracy: 0.1732 Validation Loss: 7.6429, Validation Accuracy: 0.1536\n",
      "Epoch 6/50 Training Loss: 6.5652, Training Accuracy: 0.1893 Validation Loss: 6.0681, Validation Accuracy: 0.1571\n",
      "Epoch 7/50 Training Loss: 6.8593, Training Accuracy: 0.1973 Validation Loss: 5.6262, Validation Accuracy: 0.1893\n",
      "Epoch 8/50 Training Loss: 7.4869, Training Accuracy: 0.1795 Validation Loss: 8.6659, Validation Accuracy: 0.2071\n",
      "Epoch 9/50 Training Loss: 7.5943, Training Accuracy: 0.1893 Validation Loss: 7.0307, Validation Accuracy: 0.1571\n",
      "Epoch 10/50 Training Loss: 6.6505, Training Accuracy: 0.2027 Validation Loss: 11.7451, Validation Accuracy: 0.1286\n",
      "Epoch 11/50 Training Loss: 8.4539, Training Accuracy: 0.1830 Validation Loss: 5.7925, Validation Accuracy: 0.1857\n",
      "Epoch 12/50 Training Loss: 6.8325, Training Accuracy: 0.2080 Validation Loss: 9.4540, Validation Accuracy: 0.1179\n",
      "Epoch 13/50 Training Loss: 7.2214, Training Accuracy: 0.1964 Validation Loss: 7.4409, Validation Accuracy: 0.1286\n",
      "Epoch 14/50 Training Loss: 8.8274, Training Accuracy: 0.1875 Validation Loss: 12.5927, Validation Accuracy: 0.1536\n",
      "Epoch 15/50 Training Loss: 7.7131, Training Accuracy: 0.2089 Validation Loss: 7.5127, Validation Accuracy: 0.3107\n",
      "Epoch 16/50 Training Loss: 6.5883, Training Accuracy: 0.2089 Validation Loss: 5.0753, Validation Accuracy: 0.1536\n",
      "Epoch 17/50 Training Loss: 6.7502, Training Accuracy: 0.1938 Validation Loss: 6.5291, Validation Accuracy: 0.1786\n",
      "Epoch 18/50 Training Loss: 5.0624, Training Accuracy: 0.1911 Validation Loss: 4.5919, Validation Accuracy: 0.2107\n",
      "Epoch 19/50 Training Loss: 5.6806, Training Accuracy: 0.2009 Validation Loss: 4.8772, Validation Accuracy: 0.1464\n",
      "Epoch 20/50 Training Loss: 5.0613, Training Accuracy: 0.2009 Validation Loss: 4.9277, Validation Accuracy: 0.1571\n",
      "Epoch 21/50 Training Loss: 7.1426, Training Accuracy: 0.1857 Validation Loss: 9.4666, Validation Accuracy: 0.2929\n",
      "Epoch 22/50 Training Loss: 5.9464, Training Accuracy: 0.2143 Validation Loss: 7.2414, Validation Accuracy: 0.2143\n",
      "Epoch 23/50 Training Loss: 6.1398, Training Accuracy: 0.1973 Validation Loss: 3.9130, Validation Accuracy: 0.1286\n",
      "Epoch 24/50 Training Loss: 8.2786, Training Accuracy: 0.1964 Validation Loss: 8.3805, Validation Accuracy: 0.2500\n",
      "Epoch 25/50 Training Loss: 5.1255, Training Accuracy: 0.1982 Validation Loss: 6.0610, Validation Accuracy: 0.1464\n",
      "Epoch 26/50 Training Loss: 5.0360, Training Accuracy: 0.2045 Validation Loss: 4.3335, Validation Accuracy: 0.1607\n",
      "Epoch 27/50 Training Loss: 6.1888, Training Accuracy: 0.1866 Validation Loss: 7.3550, Validation Accuracy: 0.3214\n",
      "Epoch 28/50 Training Loss: 6.2137, Training Accuracy: 0.2170 Validation Loss: 3.2559, Validation Accuracy: 0.1714\n",
      "Epoch 29/50 Training Loss: 4.4388, Training Accuracy: 0.2036 Validation Loss: 5.0672, Validation Accuracy: 0.1107\n",
      "Epoch 30/50 Training Loss: 5.8190, Training Accuracy: 0.1911 Validation Loss: 7.5662, Validation Accuracy: 0.1071\n",
      "Epoch 31/50 Training Loss: 4.7232, Training Accuracy: 0.2214 Validation Loss: 5.0606, Validation Accuracy: 0.1714\n",
      "Epoch 32/50 Training Loss: 4.3306, Training Accuracy: 0.2188 Validation Loss: 4.0109, Validation Accuracy: 0.1214\n",
      "Epoch 33/50 Training Loss: 3.8745, Training Accuracy: 0.2348 Validation Loss: 4.5480, Validation Accuracy: 0.3179\n",
      "Epoch 34/50 Training Loss: 5.4891, Training Accuracy: 0.2054 Validation Loss: 6.4951, Validation Accuracy: 0.1964\n",
      "Epoch 35/50 Training Loss: 5.0032, Training Accuracy: 0.2062 Validation Loss: 6.1212, Validation Accuracy: 0.1429\n",
      "Epoch 36/50 Training Loss: 5.8172, Training Accuracy: 0.1929 Validation Loss: 6.9074, Validation Accuracy: 0.1464\n",
      "Epoch 37/50 Training Loss: 6.3969, Training Accuracy: 0.2170 Validation Loss: 6.8862, Validation Accuracy: 0.1179\n",
      "Epoch 38/50 Training Loss: 5.6011, Training Accuracy: 0.1893 Validation Loss: 4.7114, Validation Accuracy: 0.1536\n",
      "Epoch 39/50 Training Loss: 4.8785, Training Accuracy: 0.2027 Validation Loss: 7.5965, Validation Accuracy: 0.1321\n",
      "Epoch 40/50 Training Loss: 6.0192, Training Accuracy: 0.1848 Validation Loss: 3.5892, Validation Accuracy: 0.2393\n",
      "Epoch 41/50 Training Loss: 4.2047, Training Accuracy: 0.2027 Validation Loss: 4.5763, Validation Accuracy: 0.1607\n",
      "Epoch 42/50 Training Loss: 4.9941, Training Accuracy: 0.2170 Validation Loss: 5.7773, Validation Accuracy: 0.1000\n",
      "Epoch 43/50 Training Loss: 4.3566, Training Accuracy: 0.2196 Validation Loss: 4.6731, Validation Accuracy: 0.1929\n",
      "Epoch 44/50 Training Loss: 4.3888, Training Accuracy: 0.2062 Validation Loss: 3.3306, Validation Accuracy: 0.2357\n",
      "Epoch 45/50 Training Loss: 4.3302, Training Accuracy: 0.2071 Validation Loss: 5.1825, Validation Accuracy: 0.1393\n",
      "Epoch 46/50 Training Loss: 3.7093, Training Accuracy: 0.2241 Validation Loss: 2.8508, Validation Accuracy: 0.2036\n",
      "Epoch 47/50 Training Loss: 4.1666, Training Accuracy: 0.2036 Validation Loss: 4.3053, Validation Accuracy: 0.3036\n",
      "Epoch 48/50 Training Loss: 4.0639, Training Accuracy: 0.2277 Validation Loss: 9.5206, Validation Accuracy: 0.1571\n",
      "Epoch 49/50 Training Loss: 5.2483, Training Accuracy: 0.1759 Validation Loss: 5.0432, Validation Accuracy: 0.1464\n",
      "Epoch 50/50 Training Loss: 4.0527, Training Accuracy: 0.2045 Validation Loss: 4.1578, Validation Accuracy: 0.2857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the features and labels from the CSV file\n",
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "features = df.iloc[:, :-1].values  # Assuming the last column is the label\n",
    "labels = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Convert labels to numerical values using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "features_train, features_val, labels_train, labels_val = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_train_tensor = torch.tensor(features_train, dtype=torch.float32)\n",
    "labels_train_tensor = torch.tensor(labels_train, dtype=torch.long)\n",
    "features_val_tensor = torch.tensor(features_val, dtype=torch.float32)\n",
    "labels_val_tensor = torch.tensor(labels_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(features_train_tensor, labels_train_tensor)\n",
    "val_dataset = TensorDataset(features_val_tensor, labels_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define your neural network architecture\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "input_size = features.shape[1]  # Adjust this based on the number of features\n",
    "hidden_size = 64  # You can adjust this as well\n",
    "num_classes = len(np.unique(labels))\n",
    "model = SimpleANN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "    total_loss_train = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        features_batch, labels_batch = batch\n",
    "        features_batch, labels_batch = features_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features_batch)\n",
    "        loss_train = criterion(outputs, labels_batch)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss_train.item()\n",
    "\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct_train += (predicted == labels_batch).sum().item()\n",
    "        total_samples_train += labels_batch.size(0)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_correct_val = 0\n",
    "    total_samples_val = 0\n",
    "    total_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features_batch, labels_batch = batch\n",
    "            features_batch, labels_batch = features_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "            outputs = model(features_batch)\n",
    "            loss_val = criterion(outputs, labels_batch)\n",
    "            total_loss_val += loss_val.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels_batch).sum().item()\n",
    "            total_samples += labels_batch.size(0)\n",
    "            total_correct_val += (predicted == labels_batch).sum().item()\n",
    "            total_samples_val += labels_batch.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    avg_loss_train = total_loss_train / len(train_loader)\n",
    "    accuracy_train = total_correct_train / total_samples_train\n",
    "\n",
    "    avg_loss_val = total_loss_val / len(val_loader)\n",
    "    accuracy_val = total_correct_val / total_samples_val\n",
    "\n",
    "    #print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} Training Loss: {avg_loss_train:.4f}, Training Accuracy: {accuracy_train:.4f} Validation Loss: {avg_loss_val:.4f}, Validation Accuracy: {accuracy_val:.4f}')\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(model.state_dict(), 'audio_classifier_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shami\\AppData\\Roaming\\Python\\Python311\\site-packages\\librosa\\core\\audio.py:175: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that the audio belongs to class: ['washing machine']\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('audio_classifier_model.pth'))\n",
    "\n",
    "new_audio_path = 'Recording.wav'\n",
    "\n",
    "feature = []\n",
    "audio_data,sr = librosa.load(new_audio_path)\n",
    "\n",
    "rms = librosa.feature.rms(y=audio_data)\n",
    "rms_mean = np.mean(rms,axis=1)\n",
    "rms_median = np.median(rms,axis=1)\n",
    "feature.extend([rms_mean[0],rms_median[0]])\n",
    "\n",
    "spec_bw = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr)\n",
    "spec_bw_mean = np.mean(spec_bw,axis=1)\n",
    "spec_bw_median = np.median(spec_bw,axis=1)\n",
    "feature.extend([spec_bw_mean[0],spec_bw_median[0]])\n",
    "\n",
    "S = np.abs(librosa.stft(y=audio_data))\n",
    "contrast = librosa.feature.spectral_contrast(S=S, sr=sr)\n",
    "contrast_row_mean = np.mean(np.mean(contrast,axis=1))\n",
    "contrast_col_mean = np.mean(np.mean(contrast,axis=0))\n",
    "contrast_row_median = np.median(np.median(contrast,axis=1))\n",
    "contrast_col_median = np.median(np.median(contrast,axis=0))\n",
    "feature.extend([contrast_row_mean, contrast_col_mean, contrast_row_median, contrast_col_median])\n",
    "\n",
    "chroma_cens = librosa.feature.chroma_cens(y=audio_data, sr=sr)\n",
    "chroma_cq = librosa.feature.chroma_cqt(y=audio_data, sr=sr)\n",
    "chroma_cens_row_mean = np.mean(np.mean(chroma_cens,axis=1))\n",
    "chroma_cens_col_mean = np.mean(np.mean(chroma_cens,axis=0))\n",
    "chroma_cens_row_median = np.median(np.median(chroma_cq,axis=1))\n",
    "chroma_cens_col_median = np.median(np.median(chroma_cq,axis=0))\n",
    "chroma_cq_row_mean = np.mean(np.mean(chroma_cens,axis=1))\n",
    "chroma_cq_col_mean = np.mean(np.mean(chroma_cens,axis=0))\n",
    "chroma_cq_row_median = np.median(np.median(chroma_cq,axis=1))\n",
    "chroma_cq_col_median = np.median(np.median(chroma_cq,axis=0))\n",
    "feature.extend([chroma_cens_row_mean, chroma_cens_col_mean, chroma_cens_row_median, chroma_cens_col_median, chroma_cq_row_mean, chroma_cq_col_mean, chroma_cq_row_median, chroma_cq_col_median])\n",
    "\n",
    "cent = librosa.feature.spectral_centroid(y=audio_data, sr=sr)\n",
    "cent_mean = np.mean(cent,axis=1)\n",
    "cent_median = np.median(cent,axis=1)\n",
    "feature.extend([cent_mean[0],cent_median[0]])\n",
    "\n",
    "flatness = librosa.feature.spectral_flatness(y=audio_data)\n",
    "flatness_mean = np.mean(flatness,axis=1)\n",
    "flatness_median = np.median(flatness,axis=1)\n",
    "feature.extend([flatness_mean[0],flatness_median[0]])\n",
    "\n",
    "rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)\n",
    "rolloff_mean = np.mean(rolloff,axis=1)\n",
    "rolloff_median = np.median(rolloff,axis=1)\n",
    "feature.extend([rolloff_mean[0],rolloff_median[0]])\n",
    "\n",
    "mfcc = librosa.feature.mfcc(y=audio_data,sr=sr)\n",
    "row_mean = np.mean(np.mean(mfcc,axis=1))\n",
    "col_mean = np.mean(np.mean(mfcc,axis=0))\n",
    "row_median = np.median(np.median(mfcc,axis=1))\n",
    "col_median = np.median(np.median(mfcc,axis=0))\n",
    "feature.extend([row_mean, col_mean, row_median, col_median])\n",
    "\n",
    "zcr = librosa.feature.zero_crossing_rate(y=audio_data)\n",
    "zcr_mean = np.mean(zcr,axis=1)\n",
    "zcr_median = np.median(zcr,axis=1)\n",
    "feature.extend([zcr_mean[0],zcr_median[0]])\n",
    "\n",
    "magnitude, phase = librosa.magphase(librosa.stft(y=audio_data))\n",
    "sro = librosa.feature.spectral_rolloff(S=magnitude,sr=sr)\n",
    "sro_mean = np.mean(sro,axis=1)\n",
    "sro_median = np.median(sro,axis=1)\n",
    "feature.extend([sro_mean[0],sro_median[0]])\n",
    "\n",
    "pitches,magnitudes = librosa.core.piptrack(y=audio_data,sr=sr)\n",
    "pitches_row_mean = np.mean(np.mean(pitches,axis=1))\n",
    "pitches_col_mean = np.mean(np.mean(pitches,axis=0))\n",
    "pitches_row_median = np.median(np.median(pitches,axis=1))\n",
    "pitches_col_median = np.median(np.median(pitches,axis=0))\n",
    "magnitudes_row_mean = np.mean(np.mean(magnitudes,axis=1))\n",
    "magnitudes_col_mean = np.mean(np.mean(magnitudes,axis=0))\n",
    "magnitudes_row_median = np.median(np.median(magnitudes,axis=1))\n",
    "magnitudes_col_median = np.median(np.median(magnitudes,axis=0))\n",
    "feature.extend([pitches_row_mean, pitches_col_mean, pitches_row_median, pitches_col_median, magnitudes_row_mean, magnitudes_col_mean, magnitudes_row_median, magnitudes_col_median])\n",
    "\n",
    "c_stft = librosa.feature.chroma_stft(y=audio_data,sr=sr)\n",
    "c_stft_row_mean = np.mean(np.mean(c_stft,axis=1))\n",
    "c_stft_col_mean = np.mean(np.mean(c_stft,axis=0))\n",
    "c_stft_row_median = np.median(np.median(c_stft,axis=1))\n",
    "c_stft_col_median = np.median(np.median(c_stft,axis=0))\n",
    "feature.extend([c_stft_row_mean,c_stft_col_mean,c_stft_row_median,c_stft_col_median])\n",
    "\n",
    "y = librosa.effects.harmonic(audio_data)\n",
    "tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sr)\n",
    "tonnetz_row_mean = np.mean(np.mean(tonnetz,axis=1))\n",
    "tonnetz_col_mean = np.mean(np.mean(tonnetz,axis=0))\n",
    "tonnetz_row_median = np.median(np.median(tonnetz,axis=1))\n",
    "tonnetz_col_median = np.median(np.median(tonnetz,axis=0))\n",
    "feature.extend([tonnetz_row_mean,tonnetz_col_mean,tonnetz_row_median,tonnetz_col_median])\n",
    "\n",
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "labels = df.iloc[:, -1].values\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "new_audio_features = feature\n",
    "\n",
    "new_audio_tensor = torch.tensor(new_audio_features, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_audio_tensor = new_audio_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    predictions = model(new_audio_tensor)\n",
    "    _, predicted_class = torch.max(predictions, 1)\n",
    "\n",
    "# Convert the predicted class index back to the original label\n",
    "predicted_label = label_encoder.inverse_transform([predicted_class.item()])  # Pass a list or 1D array\n",
    "\n",
    "print(f\"The model predicts that the audio belongs to class: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  4  5  6  7  8 10 11 12 13 14 15 16 20 25 28 34 35]\n"
     ]
    }
   ],
   "source": [
    "k_best = 20 # Choose the number of top features you want to select\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_new = selector.fit_transform(x, y)\n",
    "X_new.shape\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "print(selected_feature_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Dataset into Training and Tesing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_new,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# xgb = XGBClassifier(objective=\"multi:softmax\", num_class=len(set(y)), random_state=42)\n",
    "# xgb.fit(X_train, y_train)\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# xgb = LogisticRegression()\n",
    "# xgb.fit(X_train,y_train)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# xgb = RandomForestClassifier()\n",
    "# xgb.fit(X_train,y_train)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# xgb = SVC(kernel='rbf')\n",
    "# xgb.fit(X_train,y_train)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# xgb = DecisionTreeClassifier()\n",
    "# xgb.fit(X_train,y_train)\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# xgb = GaussianNB()\n",
    "# xgb.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "xgb = BernoulliNB()\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_test = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05\n",
      "18.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_train = accuracy_score(y_train,y_pred_train)\n",
    "acc_test = accuracy_score(y_test,y_pred_test)\n",
    "print(round(acc_train*100,2))\n",
    "print(round(acc_test*100,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
